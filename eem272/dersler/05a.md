<!-- ---
marp: true
--- -->

Formülleri düzgün gösterildiği sayfa için [tıklayınız](https://github.com/asenturk/asenturk.github.io/blob/main/eem272/dersler/05a.md).

# Gradient Descent (Gradyan İnişi) Nedir?

Gradient Descent, bir makine öğrenimi modelinin hata oranını en aza indirmek için kullanılan optimizasyon algoritmasıdır.

Amaç: Modelin tahmin hatasını düşürmek için ağırlıkları güncelleyerek en düşük kayıp fonksiyonunu bulmak.
Nasıl çalışır?

- Ağırlıklar belirlenir.
- Kaybın (hata oranının) ne kadar büyük olduğu hesaplanır.
- Hata en aza inene kadar ağırlıklar küçük adımlarla güncellenir.
- Öğrenme oranı ($\alpha$) güncelleme hızını belirler.

---

# Batch Gradient Descent (Veri Kümesi Gradyan İnişi)

- Tüm eğitim verisi kullanılır.
- Her iterasyonda tüm veri kümesiyle hesaplama yapılır.
- Kesin ama yavaş bir yöntemdir.


- Avantajları:
  - Daha doğru gradyan hesaplaması yapar.
  - Küçük veri kümelerinde iyi çalışır.

- Dezavantajları:
  - Büyük veri kümelerinde çok yavaş olur.
  - Bellek kullanımı yüksektir.

---

# Stochastic Gradient Descent (SGD - Tekil Örnekli Gradyan İnişi)

- Her iterasyonda yalnızca bir veri örneğiyle hesaplama yapılır.
- Daha hızlıdır ama daha az kararlıdır.
- Hata yüzeyi üzerinde daha rastgele hareket eder.

- Avantajları:
  - Az bellek kullanır. Hızlıdır.

- Dezavantajları:
  - Gradyanlar düzensiz hareket eder, bazen yanlış yönde ilerleyebilir.
  - Daha fazla dalgalanma (oscillation) olabilir.

---


# Mini-Batch Gradient Descent (Küçük Veri Kümesi Gradyan İnişi)

- Tüm veri yerine küçük gruplar (mini-batches) halinde çalışır.
- Batch ve SGD yöntemlerinin en iyi yönlerini birleştirir.
- Dengeyi sağlayan en yaygın kullanılan yöntemdir!



- Avantajları:
  - Bellek kullanımı batch yöntemine göre daha iyidir.
  - SGD'ye göre daha kararlı sonuçlar verir.
  - Paralel hesaplama ile hızlandırılabilir.

- Dezavantajları:
  - Batch kadar doğru olmayabilir.
  - Mini-batch boyutu iyi seçilmezse performans düşebilir.

---

# Hangi Yöntemi Kullanmalıyız?

- Küçük veri setleri için → Batch Gradient Descent
- Gerçek zamanlı öğrenme için → SGD
- Büyük veri ve dengeli performans için → Mini-Batch Gradient Descent

---

# Epoch

- Modelin tüm verilerle 1 kez eğitilmesine 1 epoch denilir.
- Örneğin veri sayısı 10.000, batch size 1000 epoch 5 ise
  - her 1000 veride 1 geri yayılım yapılır.
  - toplam 50 geri yayılım 
- Epoch sayısı çok düşükse, model yetersiz öğrenir (underfitting).
- Epoch sayısı çok yüksekse, model ezberleyebilir (overfitting).
- Genellikle erken durdurma (early stopping) gibi yöntemlerle en uygun epoch sayısı bulunur.

---
# Parametre - Hiperparametre

- **Parametreler** model tarafından eğitim esnasında öğrenilen değerlerdir.
- Bir yapay sinir ağında ağırlıklar (weights) ve biaslar parametrelerdir.
  - $y=w_1x_1​+w_2​x_2​+b$
- **Hiperparametreler** model eğitilmeden önce belirlenen ayarlardır.
  - Eğitim sürecini ve modelin öğrenme şeklini etkiler.
  - Elle ayarlanır (deneme-yanılma veya optimizasyon teknikleri ile seçilir).
  - Öğrenme oranı 
  - Epoch sayısı, Batch boyutu
  - Katman sayısı ve nöron sayısı
  - Aktivasyon fonksiyonu

---

# Sınıflandırmada Aktivasyon ve Kayıp Fonksiyonları
## Sigmoid Aktivasyon Fonksiyonu
- Sigmoid fonksiyonu, çıkışı 0 ile 1 arasında bir olasılık değeri olarak verir.

- $\sigma(x) = \frac{1}{1 + e^{-x}}$

- Olasılık tahmini yapar
- Büyük değerlerde gradyan sıfıra yaklaşabilir (vanishing gradient)

---
## Softmax Aktivasyon Fonksiyonu
- Softmax, birden fazla sınıfın olasılıklarını hesaplar ve toplamı 1 olacak şekilde normalize eder.

$S_i = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$


---

## Binary Crossentropy (İkili Çapraz Entropi )
- İki sınıflı (binary) problemler için modelin hata oranını hesaplar.

$L = - (y \log(p) + (1 - y) \log(1 - p))$

---

## Categorical Crossentropy (Kategorik Çapraz Entropi Kayıp Fonksiyonu)

- Birden fazla sınıf olan (multi-class) problemler için kullanılır.
- One-hot encoding ile çalışır.

$L = -\sum_{i} y_i \log(p_i)$



---

# **Sınıflandırma Metrikleri ve Formülleri**

## **TP, TN, FP, FN Kavramları**
- True Positive (TP) - Gerçek Pozitif
  - Modelin "Pozitif" tahmin ettiği ve gerçekten pozitif olan örneklerdir.
- True Negative (TN) - Gerçek Negatif
  - Modelin "Negatif" tahmin ettiği ve gerçekten negatif olan örneklerdir.
- False Positive (FP) - Yanlış Pozitif
  - Modelin "Pozitif" tahmin ettiği ama aslında negatif olan örneklerdir.
- False Negative (FN) - Yanlış Negatif
  -Modelin "Negatif" tahmin ettiği ama aslında pozitif olan örneklerdir.


---



**Bir sınıflandırma modelinin performansını değerlendirmek için kullanılan temel terimler:**

| Gerçek \ Tahmin | Pozitif (P) | Negatif (N) |
|---------------|------------|------------|
| **Pozitif (P)** | ✅ **TP (True Positive)** | ❌ **FN (False Negative)** |
| **Negatif (N)** | ❌ **FP (False Positive)** | ✅ **TN (True Negative)** |


**Örnek**
| Durum | TP | TN | FP | FN |
|-------|----|----|----|----|
| **Spam Filtresi** | Spam e-postayı "Spam" olarak işaretlemek | Normal e-postayı "Normal" olarak bırakmak | Normal e-postayı "Spam" olarak yanlış işaretlemek | Spam e-postayı "Normal" olarak kaçırmak |



---

# Metrikler
## 1️⃣ Doğruluk (Accuracy)
Doğru tahmin edilen örneklerin, toplam örnek sayısına oranıdır.

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$


## 2️⃣ Kesinlik (Precision)
Bir sınıfa ait olarak tahmin edilen örneklerin, gerçekten o sınıfa ait olma oranıdır.

$$
Precision = \frac{TP}{TP + FP}
$$

---

## 3️⃣ Duyarlılık (Recall)
Gerçekten pozitif olan örneklerin, model tarafından doğru tahmin edilme oranıdır.

$$
Recall = \frac{TP}{TP + FN}
$$



## 4️⃣ F1-Skoru
Kesinlik ve duyarlılığın harmonik ortalamasıdır. Dengesiz veri kümelerinde kullanışlıdır.

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

---

## 5️⃣ Karıştırma Matrisi (Confusion Matrix)
Her sınıfın gerçek ve tahmin edilen değerleri arasındaki ilişkileri gösteren matristir.

| Gerçek \ Tahmin | Pozitif (P) | Negatif (N) |
|---------------|------------|------------|
| **Pozitif (P)** | TP (True Positive) | FN (False Negative) |
| **Negatif (N)** | FP (False Positive) | TN (True Negative) |

---

## Örnek Hesaplama
Eğer bir model:
- **Gerçek pozitif (TP) = 90**
- **Gerçek negatif (TN) = 50**
- **Yanlış pozitif (FP) = 30**
- **Yanlış negatif (FN) = 20**  

için hesaplamalar:

- **Doğruluk (Accuracy)**:  
$$
Accuracy = \frac{90 + 50}{90 + 50 + 30 + 20} = 0.74 
$$

---

- **Kesinlik (Precision)**:  
$$
Precision = \frac{90}{90 + 30} = 0.75 
$$

- **Duyarlılık (Recall)**:  
$$
Recall = \frac{90}{90 + 20} = 0.818 
$$

- **F1-Skoru**:  
$$
F1 = 2 \times \frac{0.75 \times 0.818}{0.75 + 0.818} = 0.783
$$